{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# basic python package\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark related\n",
    "from pyspark.sql import DataFrameWriter\n",
    "sql = sqlContext.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'local[*]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark context\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: ../../dataset/profile: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../../dataset/profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "profile_path = os.path.abspath('../dataset/profile/**/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1,1083060338,F,I,292141,F,606063,3667420800,173.0289647440402,168.62583605184867,Y,Y,Y,Y,3696969600',\n",
       " u'2,2186820338,F,B,653673,B,122851,3291321600,200.84809884772136,1004.8535938607777,Y,Y,N,Y,3696969600',\n",
       " u'3,1966068338,M,D,373933,D,639201,3373574400,1589.205135410834,58.94475293467117,N,Y,N,N,3696969600',\n",
       " u'4,1650708338,M,D,39007,H,87913,3402950400,415.73561935499026,183.66420164840252,Y,Y,N,Y,3696969600',\n",
       " u'5,1335348338,F,G,254521,F,472657,3312921600,1225.8381529866247,3330.3261064497733,Y,Y,Y,Y,3696969600']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_text = sc.textFile(profile_path)\n",
    "profile_text.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data to Spark Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+-----------+------------+------------+-------------+----------+------------------+------------------+----------------+---------+------------+-----------+--------------+\n",
      "|customer_id|birth_time|gender|contact_loc|contact_code|register_loc|register_code|start_time|               aum|        net_profit|credit_card_flag|loan_flag|deposit_flag|wealth_flag|partition_time|\n",
      "+-----------+----------+------+-----------+------------+------------+-------------+----------+------------------+------------------+----------------+---------+------------+-----------+--------------+\n",
      "|          1|1083060338|     F|          I|      292141|           F|       606063|3667420800| 173.0289647440402|168.62583605184867|               Y|        Y|           Y|          Y|    3696969600|\n",
      "|          2|2186820338|     F|          B|      653673|           B|       122851|3291321600|200.84809884772136|1004.8535938607777|               Y|        Y|           N|          Y|    3696969600|\n",
      "|          3|1966068338|     M|          D|      373933|           D|       639201|3373574400| 1589.205135410834| 58.94475293467117|               N|        Y|           N|          N|    3696969600|\n",
      "|          4|1650708338|     M|          D|       39007|           H|        87913|3402950400|415.73561935499026|183.66420164840252|               Y|        Y|           N|          Y|    3696969600|\n",
      "|          5|1335348338|     F|          G|      254521|           F|       472657|3312921600|1225.8381529866247|3330.3261064497733|               Y|        Y|           Y|          Y|    3696969600|\n",
      "+-----------+----------+------+-----------+------------+------------+-------------+----------+------------------+------------------+----------------+---------+------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "fields = [\n",
    "    StructField('customer_id', StringType(), False),\n",
    "    StructField('birth_time', LongType(), True),\n",
    "    StructField('gender', StringType(), True),\n",
    "    StructField('contact_loc', StringType(), True),\n",
    "    StructField('contact_code', StringType(), True),\n",
    "    StructField('register_loc', StringType(), True),\n",
    "    StructField('register_code', StringType(), True),\n",
    "    StructField('start_time', LongType(), True),\n",
    "    StructField('aum', DoubleType(), True),\n",
    "    StructField('net_profit', DoubleType(), True),\n",
    "    StructField('credit_card_flag', StringType(), True),\n",
    "    StructField('loan_flag', StringType(), True),\n",
    "    StructField('deposit_flag', StringType(), True),\n",
    "    StructField('wealth_flag', StringType(), True),\n",
    "    StructField('partition_time', LongType(), True)\n",
    "]\n",
    "\n",
    "schema = StructType(fields)\n",
    "\n",
    "def process_profile_line(l):\n",
    "    p = l.split(',')\n",
    "    # for LongType\n",
    "    for i in [1, 7, 14]:\n",
    "        p[i] = int(p[i])\n",
    "    # for DoubleType\n",
    "    for i in [8, 9]:\n",
    "        p[i] = float(p[i])\n",
    "    return p\n",
    "\n",
    "profile_df = profile_text \\\n",
    "    .map(process_profile_line) \\\n",
    "    .toDF(schema=schema)\n",
    "    \n",
    "profile_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event - CCTXN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'customer_id,4,purchase_consumption,3696975741,merchant_nbr,VUAFPKBFAIENYETOLYIX,credit_card,GPSHLCABRIQBVOKVCUNK,\"{\\\\\"action\\\\\": {\\\\\"txn_amt\\\\\": 1173.4403405275684, \\\\\"original_currency_code\\\\\": \\\\\"TWD\\\\\"}, \\\\\"object\\\\\": {\\\\\"merchant_category_code\\\\\": \\\\\"05971\\\\\"}, \\\\\"channel\\\\\": {\\\\\"card_type\\\\\": \\\\\"B\\\\\", \\\\\"card_level\\\\\": \\\\\"5\\\\\"}}\",3696969600,cc_txn',\n",
       " u'customer_id,29,purchase_consumption,3696975741,merchant_nbr,VUAFPKBFAIENYETOLYIX,credit_card,GPSHLCABRIQBVOKVCUNK,\"{\\\\\"action\\\\\": {\\\\\"txn_amt\\\\\": 1173.4403405275684, \\\\\"original_currency_code\\\\\": \\\\\"TWD\\\\\"}, \\\\\"object\\\\\": {\\\\\"merchant_category_code\\\\\": \\\\\"05971\\\\\"}, \\\\\"channel\\\\\": {\\\\\"card_type\\\\\": \\\\\"B\\\\\", \\\\\"card_level\\\\\": \\\\\"5\\\\\"}}\",3696969600,cc_txn']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cctxn_text = sc.textFile(os.path.abspath('../dataset/cctxn/**/*.csv'))\n",
    "cctxn_text.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------------------+-----------+------------+--------------------+------------+--------------------+--------------------+--------------+------+\n",
      "| actor_type|actor_id|         action_type|action_time| object_type|           object_id|channel_type|          channel_id|               attrs|partition_time| theme|\n",
      "+-----------+--------+--------------------+-----------+------------+--------------------+------------+--------------------+--------------------+--------------+------+\n",
      "|customer_id|       4|purchase_consumption| 3696975741|merchant_nbr|VUAFPKBFAIENYETOLYIX| credit_card|GPSHLCABRIQBVOKVCUNK|{\"action\": {\"txn_...|    3696969600|cc_txn|\n",
      "|customer_id|      29|purchase_consumption| 3696975741|merchant_nbr|VUAFPKBFAIENYETOLYIX| credit_card|GPSHLCABRIQBVOKVCUNK|{\"action\": {\"txn_...|    3696969600|cc_txn|\n",
      "|customer_id|      18|purchase_consumption| 3696975866|merchant_nbr|HJMGNAQEFRZMCIDBHLSR| credit_card|GOBOOQKAPKTNJLDLXXDI|{\"action\": {\"txn_...|    3696969600|cc_txn|\n",
      "|customer_id|      43|purchase_consumption| 3696975866|merchant_nbr|HJMGNAQEFRZMCIDBHLSR| credit_card|GOBOOQKAPKTNJLDLXXDI|{\"action\": {\"txn_...|    3696969600|cc_txn|\n",
      "|customer_id|      15|purchase_consumption| 3696976193|merchant_nbr|AQPRQWDZXOLHYVJBTWFY| credit_card|WSBGQLGVXQBVEDZRVODM|{\"action\": {\"txn_...|    3696969600|cc_txn|\n",
      "+-----------+--------+--------------------+-----------+------------+--------------------+------------+--------------------+--------------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "fields = [\n",
    "    StructField('actor_type', StringType(), False),\n",
    "    StructField('actor_id', StringType(), True),\n",
    "    StructField('action_type', StringType(), True),\n",
    "    StructField('action_time', LongType(), True),\n",
    "    StructField('object_type', StringType(), True),\n",
    "    StructField('object_id', StringType(), True),\n",
    "    StructField('channel_type', StringType(), True),\n",
    "    StructField('channel_id', StringType(), True),\n",
    "    StructField('attrs', StringType(), True),\n",
    "    StructField('partition_time', LongType(), True),\n",
    "    StructField('theme', StringType(), True),\n",
    "]\n",
    "\n",
    "schema = StructType(fields)\n",
    "\n",
    "def process_profile_line(l):\n",
    "    p = l.split(',')\n",
    "    # pre\n",
    "    pre_fields = p[:8]\n",
    "    pre_fields[3] = int(pre_fields[3])\n",
    "    # post\n",
    "    post_fields = p[-2:]\n",
    "    post_fields[-2] = int(post_fields[-2])\n",
    "    # attrs\n",
    "    attrs = json.loads(','.join(p[8:-2]))\n",
    "    # concat fields\n",
    "    fields = pre_fields + [attrs] + post_fields\n",
    "    return fields\n",
    "\n",
    "cctxn_df = cctxn_text.map(process_profile_line).toDF(schema=schema)\n",
    "\n",
    "cctxn_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Use JSON process \"attrs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'action': {u'original_currency_code': u'TWD',\n",
       "   u'txn_amt': 1173.4403405275684},\n",
       "  u'channel': {u'card_level': u'5', u'card_type': u'B'},\n",
       "  u'object': {u'merchant_category_code': u'05971'}},\n",
       " {u'action': {u'original_currency_code': u'TWD',\n",
       "   u'txn_amt': 1173.4403405275684},\n",
       "  u'channel': {u'card_level': u'5', u'card_type': u'B'},\n",
       "  u'object': {u'merchant_category_code': u'05971'}},\n",
       " {u'action': {u'original_currency_code': u'TWD',\n",
       "   u'txn_amt': 177.63228741462754},\n",
       "  u'channel': {u'card_level': u'3', u'card_type': u'B'},\n",
       "  u'object': {u'merchant_category_code': u'03723'}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# return to RDD for unstructured data\n",
    "cctxn_attrs = cctxn_df.select(\"attrs\").rdd.map(lambda r: r.attrs).map(lambda x: json.loads(x))\n",
    "cctxn_attrs.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of each card level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'3', 624), (u'4', 566), (u'5', 460), (u'2', 334), (u'6', 224), (u'1', 134)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cctxn_attrs.filter(lambda x: x['channel']['card_level'] is not None) \\\n",
    "    .map(lambda x: x['channel']['card_level']) \\\n",
    "    .map(lambda cl: (cl, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .sortBy(lambda x: -x[1]) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data from different theme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-----------+-----------+-----------+--------------------+------------+--------------------+--------------------+--------------+-----+\n",
      "| actor_type|actor_id|action_type|action_time|object_type|           object_id|channel_type|          channel_id|               attrs|partition_time|theme|\n",
      "+-----------+--------+-----------+-----------+-----------+--------------------+------------+--------------------+--------------------+--------------+-----+\n",
      "|customer_id|      24|   transfer| 3696969655|saving_acct|RRQAIWQTVFEMFNHQRAKO|         ATM|BGZXUBVPWOQPFUBINYEW|{\"action\": {\"txn_...|    3696969600|  atm|\n",
      "|customer_id|      49|   transfer| 3696969655|saving_acct|RRQAIWQTVFEMFNHQRAKO|         ATM|BGZXUBVPWOQPFUBINYEW|{\"action\": {\"txn_...|    3696969600|  atm|\n",
      "|customer_id|       6|   withdraw| 3696971685|saving_acct|SICMEBNJDHPWWCVZJMUS|         ATM|VMZDTVFCDQNGPUHYFVBP|{\"action\": {\"txn_...|    3696969600|  atm|\n",
      "|customer_id|      31|   withdraw| 3696971685|saving_acct|SICMEBNJDHPWWCVZJMUS|         ATM|VMZDTVFCDQNGPUHYFVBP|{\"action\": {\"txn_...|    3696969600|  atm|\n",
      "|customer_id|      19|   withdraw| 3696983296|saving_acct|XTXFJVXEYQZOWNWNZVXJ|         ATM|GWDIGLGAJYUCPLLPKFYS|{\"action\": {\"txn_...|    3696969600|  atm|\n",
      "+-----------+--------+-----------+-----------+-----------+--------------------+------------+--------------------+--------------------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "atm_df = sc.textFile(os.path.abspath('../dataset/atm/**/*.csv')) \\\n",
    "    .map(process_profile_line) \\\n",
    "    .toDF(schema=schema)\n",
    "\n",
    "atm_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-----------+-----------+------------+---------+----------------+--------------------+--------------------+--------------+-----+\n",
      "| actor_type|actor_id|action_type|action_time| object_type|object_id|    channel_type|          channel_id|               attrs|partition_time|theme|\n",
      "+-----------+--------+-----------+-----------+------------+---------+----------------+--------------------+--------------------+--------------+-----+\n",
      "|customer_id|       2|    inbound| 3696974030|call_purpose| 2_19_162|customer_service|personal_phone_se...|{\"action\": {\"call...|    3696969600|  cti|\n",
      "|customer_id|      27|    inbound| 3696974030|call_purpose| 2_19_162|customer_service|personal_phone_se...|{\"action\": {\"call...|    3696969600|  cti|\n",
      "|customer_id|       5|    inbound| 3696978134|call_purpose| 1_18_224|customer_service|personal_phone_se...|{\"action\": {\"call...|    3696969600|  cti|\n",
      "|customer_id|      30|    inbound| 3696978134|call_purpose| 1_18_224|customer_service|personal_phone_se...|{\"action\": {\"call...|    3696969600|  cti|\n",
      "|customer_id|       5|    inbound| 3696987105|call_purpose|  0_2_167|customer_service|personal_phone_se...|{\"action\": {\"call...|    3696969600|  cti|\n",
      "+-----------+--------+-----------+-----------+------------+---------+----------------+--------------------+--------------------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cti_df = sc.textFile(os.path.abspath('../dataset/cti/**/*.csv')) \\\n",
    "    .map(process_profile_line) \\\n",
    "    .toDF(schema=schema)\n",
    "\n",
    "cti_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MyBank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------------------+-----------+-------------------+--------------------+------------+----------+--------------------+--------------+------+\n",
      "| actor_type|actor_id|       action_type|action_time|        object_type|           object_id|channel_type|channel_id|               attrs|partition_time| theme|\n",
      "+-----------+--------+------------------+-----------+-------------------+--------------------+------------+----------+--------------------+--------------+------+\n",
      "|customer_id|      14|TWD_demand_deposit| 3696977211|credit_card_payment|ZXPQYYTMNBEQMAXCVXNI|     digital|       MMB|{\"action\": {\"amt\"...|    3696969600|mybank|\n",
      "|customer_id|      39|TWD_demand_deposit| 3696977211|credit_card_payment|ZXPQYYTMNBEQMAXCVXNI|     digital|       MMB|{\"action\": {\"amt\"...|    3696969600|mybank|\n",
      "|customer_id|      23|              fund| 3696978460|   general_purchase|PKYEOQSCYIJQROCCDIJX|     digital|       MMB|{\"action\": {\"amt\"...|    3696969600|mybank|\n",
      "|customer_id|      48|              fund| 3696978460|   general_purchase|PKYEOQSCYIJQROCCDIJX|     digital|       MMB|{\"action\": {\"amt\"...|    3696969600|mybank|\n",
      "|customer_id|      12|              fund| 3696980372|   general_purchase|HJMLYNLMTJMTUYBEKXSA|     digital|    MyBank|{\"action\": {\"amt\"...|    3696969600|mybank|\n",
      "+-----------+--------+------------------+-----------+-------------------+--------------------+------------+----------+--------------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mybank_df = sc.textFile(os.path.abspath('../dataset/mybank/**/*.csv')) \\\n",
    "    .map(process_profile_line) \\\n",
    "    .toDF(schema=schema)\n",
    "\n",
    "mybank_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "event_df = cctxn_df.union(atm_df).union(cti_df).union(mybank_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5612"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "event_cnt_df = event_df \\\n",
    "    .groupBy('actor_id') \\\n",
    "    .pivot('theme', ['cc_txn', 'atm', 'cti', 'mybank']) \\\n",
    "    .count() \\\n",
    "    .na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+---+------+\n",
      "|actor_id|cc_txn|atm|cti|mybank|\n",
      "+--------+------+---+---+------+\n",
      "|       7|    48| 67|  7|    10|\n",
      "|      15|    80| 43|  4|    37|\n",
      "|      11|    97| 25| 17|     8|\n",
      "|      29|    98| 40|  7|    14|\n",
      "|      42|     0|  1| 17|    34|\n",
      "+--------+------+---+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "event_cnt_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile with Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3696969600, 3699475200]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = profile_df \\\n",
    "    .select('partition_time') \\\n",
    "    .distinct() \\\n",
    "    .orderBy('partition_time') \\\n",
    "    .rdd.map(lambda x: x.partition_time) \\\n",
    "    .collect()\n",
    "\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_profile_df = profile_df.where('partition_time={}'.format(times[0]))\n",
    "end_profile_df = profile_df.where('partition_time={}'.format(times[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+-----------+------------+------------+-------------+----------+------------------+------------------+----------------+---------+------------+-----------+--------------+\n",
      "|customer_id|birth_time|gender|contact_loc|contact_code|register_loc|register_code|start_time|               aum|        net_profit|credit_card_flag|loan_flag|deposit_flag|wealth_flag|partition_time|\n",
      "+-----------+----------+------+-----------+------------+------------+-------------+----------+------------------+------------------+----------------+---------+------------+-----------+--------------+\n",
      "|          1|1083060338|     F|          I|      292141|           F|       606063|3667420800| 173.0289647440402|168.62583605184867|               Y|        Y|           Y|          Y|    3696969600|\n",
      "|          2|2186820338|     F|          B|      653673|           B|       122851|3291321600|200.84809884772136|1004.8535938607777|               Y|        Y|           N|          Y|    3696969600|\n",
      "|          3|1966068338|     M|          D|      373933|           D|       639201|3373574400| 1589.205135410834| 58.94475293467117|               N|        Y|           N|          N|    3696969600|\n",
      "|          4|1650708338|     M|          D|       39007|           H|        87913|3402950400|415.73561935499026|183.66420164840252|               Y|        Y|           N|          Y|    3696969600|\n",
      "|          5|1335348338|     F|          G|      254521|           F|       472657|3312921600|1225.8381529866247|3330.3261064497733|               Y|        Y|           Y|          Y|    3696969600|\n",
      "+-----------+----------+------+-----------+------------+------------+-------------+----------+------------------+------------------+----------------+---------+------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_profile_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_profile_df.registerTempTable('start_profile')\n",
    "end_profile_df.registerTempTable('end_profile')\n",
    "event_cnt_df.registerTempTable('event_cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql(\"\"\"\n",
    "    select \n",
    "        a.*,\n",
    "        p1.credit_card_flag as start_cc,\n",
    "        p1.loan_flag as start_loan,\n",
    "        p1.deposit_flag as start_deposit,\n",
    "        p1.wealth_flag as start_wealth,\n",
    "        p2.credit_card_flag as end_cc,\n",
    "        p2.loan_flag as end_loan,\n",
    "        p2.deposit_flag as end_deposit,\n",
    "        p2.wealth_flag as end_wealth\n",
    "    from event_cnt a\n",
    "    join start_profile p1\n",
    "        on a.actor_id=p1.customer_id\n",
    "    join end_profile p2\n",
    "        on a.actor_id=p2.customer_id\n",
    "    order by actor_id\n",
    "\"\"\").registerTempTable('actor_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+---+------+--------+----------+-------------+------------+------+--------+-----------+----------+\n",
      "|actor_id|cc_txn|atm|cti|mybank|start_cc|start_loan|start_deposit|start_wealth|end_cc|end_loan|end_deposit|end_wealth|\n",
      "+--------+------+---+---+------+--------+----------+-------------+------------+------+--------+-----------+----------+\n",
      "|       1|     6|  8|  8|     6|       Y|         Y|            Y|           Y|     Y|       Y|          Y|         N|\n",
      "|      10|    74| 15| 13|     3|       N|         Y|            N|           N|     N|       N|          Y|         Y|\n",
      "|      11|    97| 25| 17|     8|       Y|         N|            Y|           Y|     Y|       N|          Y|         Y|\n",
      "|      12|    96| 54| 18|    27|       Y|         Y|            Y|           N|     Y|       Y|          N|         N|\n",
      "|      13|     8|  6|  6|     8|       Y|         Y|            Y|           Y|     Y|       Y|          Y|         N|\n",
      "|      14|    13| 29| 14|    24|       Y|         Y|            N|           Y|     Y|       Y|          N|         Y|\n",
      "|      15|    80| 43|  4|    37|       N|         Y|            N|           N|     Y|       N|          N|         N|\n",
      "|      16|    23| 23| 19|    32|       Y|         Y|            N|           Y|     N|       Y|          N|         N|\n",
      "|      17|  null|  1| 17|    34|       Y|         Y|            Y|           Y|     N|       Y|          Y|         N|\n",
      "|      18|    45| 64| 18|     2|       Y|         N|            N|           N|     N|       N|          Y|         N|\n",
      "+--------+------+---+---+------+--------+----------+-------------+------------+------+--------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql(\"\"\"\n",
    "    select * from actor_status\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+--------------------+------------------+--------------------+\n",
      "|actor_id|       cctxn_score|           atm_score|         cti_score|        mybank_score|\n",
      "+--------+------------------+--------------------+------------------+--------------------+\n",
      "|       1|1.7917761357558344|  2.0794540416017115|2.0794540416017115|  1.7917761357558344|\n",
      "|      10| 4.304066444554608|  2.7080568677466546|2.5649570497396432|     1.0986456214459|\n",
      "|      11| 4.574712009430686|   3.218879824860201| 2.833219226391856|  2.0794540416017115|\n",
      "|      12|  4.56434923313396|  3.9889858984144118| 2.890377313436288|   3.295840569701174|\n",
      "|      13|2.0794540416017115|  1.7917761357558344|1.7917761357558344|  2.0794540416017115|\n",
      "|      14|2.5649570497396432|   3.367299278256391|2.6390644724468912|   3.178057997005932|\n",
      "|      15|   4.3820278846731|   3.761202441272254|1.3863193608073958|   3.610920615343275|\n",
      "|      16| 3.135498563745785|   3.135498563745785| 2.944444242310485|   3.465739027794844|\n",
      "|      17|              null|9.999500033329732E-5| 2.833219226391856|   3.526363465788307|\n",
      "|      18|3.8066647119900727|   4.158884645858452| 2.890377313436288|   0.693197179309987|\n",
      "|      19|3.8066647119900727|   3.178057997005932| 2.302595092944046|                null|\n",
      "|       2|2.4849149830866115|  3.8712030942390543| 1.945924434667559|  2.0794540416017115|\n",
      "|      20| 3.433990430286395|   4.394450389239578|   1.0986456214459|9.999500033329732E-5|\n",
      "|      21|3.5553509186281893|   4.248496670619767|  2.77259497222025|   3.178057997005932|\n",
      "|      22| 4.189656257176793|   2.944444242310485|2.7080568677466546|   2.833219226391856|\n",
      "|      23|2.6390644724468912|   2.302595092944046|2.5649570497396432|   3.295840569701174|\n",
      "|      24|1.3863193608073958|   4.430817989318795| 1.609457912234103|   1.945924434667559|\n",
      "|      25|  4.02535347644784|  3.9512456416565014|2.5649570497396432|  2.0794540416017115|\n",
      "|      26|1.7917761357558344|  2.0794540416017115|2.0794540416017115|  1.7917761357558344|\n",
      "|      27|2.4849149830866115|  3.8712030942390543| 1.945924434667559|  2.0794540416017115|\n",
      "+--------+------------------+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = sql(\"\"\"\n",
    "    select \n",
    "        actor_id,\n",
    "        log(cast(cc_txn as double) + 0.0001) as cctxn_score,\n",
    "        log(cast(atm as double) + 0.0001) as atm_score,\n",
    "        log(cast(cti as double) + 0.0001) as cti_score,\n",
    "        log(cast(mybank as double) + 0.0001) as mybank_score\n",
    "    from actor_status\n",
    "\"\"\")\n",
    "\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering + KMeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o213.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 53.0 failed 1 times, most recent failure: Lost task 10.0 in stage 53.0 (TID 7245, localhost): org.apache.spark.SparkException: Values to assemble cannot be null.\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:159)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:142)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:142)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:98)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:97)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1093)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1093)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1094)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1094)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:766)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:766)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:983)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:965)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1108)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1085)\n\tat org.apache.spark.mllib.feature.StandardScaler.fit(StandardScaler.scala:57)\n\tat org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Values to assemble cannot be null.\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:159)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:142)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:142)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:98)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:97)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1093)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1093)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1094)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1094)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:766)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:766)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-dbde8cc8f36e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massembler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpipelineModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/roger19890107/Developer/main/resources/spark/spark-2.0.0-bin-hadoop2.6/python/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/Users/roger19890107/Developer/main/resources/spark/spark-2.0.0-bin-hadoop2.6/python/pyspark/ml/pipeline.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roger19890107/Developer/main/resources/spark/spark-2.0.0-bin-hadoop2.6/python/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/Users/roger19890107/Developer/main/resources/spark/spark-2.0.0-bin-hadoop2.6/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roger19890107/Developer/main/resources/spark/spark-2.0.0-bin-hadoop2.6/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \"\"\"\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roger19890107/Developer/main/resources/spark/spark-2.0.0-bin-hadoop2.6/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roger19890107/Developer/main/resources/spark/spark-2.0.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roger19890107/Developer/main/resources/spark/spark-2.0.0-bin-hadoop2.6/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o213.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 53.0 failed 1 times, most recent failure: Lost task 10.0 in stage 53.0 (TID 7245, localhost): org.apache.spark.SparkException: Values to assemble cannot be null.\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:159)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:142)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:142)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:98)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:97)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1093)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1093)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1094)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1094)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:766)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:766)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:983)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:965)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1108)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1085)\n\tat org.apache.spark.mllib.feature.StandardScaler.fit(StandardScaler.scala:57)\n\tat org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Values to assemble cannot be null.\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:159)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:142)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:142)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:98)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:97)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1093)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1093)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1094)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1094)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:766)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:766)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "cluster_num = 2\n",
    "feature_cols = ['cctxn_score', 'atm_score', 'cti_score', 'mybank_score']\n",
    "\n",
    "# Combine selected columns to generate vector column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol='features')\n",
    "\n",
    "# Standard Scaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
    "                       withStd=True, withMean=True)\n",
    "\n",
    "# KMeans\n",
    "kmeans = KMeans(k=cluster_num, featuresCol=\"scaled_features\", seed=1)\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "pipelineModel = pipeline.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans_model = pipelineModel.stages[2]\n",
    "centers = kmeans_model.clusterCenters()\n",
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_df = pipelineModel.transform(dataset)\n",
    "predicted_df.select(['actor_id', 'cctxn_score', 'atm_score', 'cti_score', 'mybank_score', 'prediction']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = predicted_df.rdd.map(lambda r: r.scaled_features.toArray()).collect()\n",
    "y = predicted_df.rdd.map(lambda r: r.prediction).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "colors = cm.rainbow(np.linspace(0, 1, cluster_num))\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "points = tsne.fit_transform(X)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel('comp 1')\n",
    "plt.ylabel('comp 2')\n",
    "for p_c in zip(points,  colors[y]):\n",
    "    plt.scatter(p_c[0][0], p_c[0][1], color=p_c[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_df.write.save(\"df-result\", mode=\"overwrite\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrameReader\n",
    "\n",
    "loaded_df = spark.read.load(\"df-result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks for your attending HackNTU X Cathay 2017!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
